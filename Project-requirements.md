# Sample exercise

## Getting started

**Exercise**: Data Pipeline Optimization

**Scenario**: You have been given a dataset in CSV format containing about a million records (raw_data.csv). Your task is to develop a data pipeline using Python and any additional libraries or tools you find necessary to efficiently process and transform the data. The pipeline should load the data, perform data cleansing, apply specific transformations, and store the transformed data in a cloud-based data warehouse.

**Note**: Make sure to anonymize any sensitive information and focus on the technical aspects of the exercise.

**Estimated Time**: This exercise is expected to take approximately 4 to 8 hours to complete, depending on your level of experience and familiarity with the tools and technologies used.

## Requirements
1. Write a Python script that reads the CSV file and performs the following transformations on the data: 
    1. Remove any duplicate records. 
    2. Convert all string values to lowercase. 
    3. Remove any leading or trailing whitespaces from string values.
2. Implement parallel processing techniques to optimize the data transformation process.
3. Utilize any relevant libraries or tools to improve the efficiency and performance of the pipeline.
4. Store the transformed data in a cloud-based data warehouse of your choice (e.g., AWS Redshift, Google BigQuery, Azure Synapse Analytics) using the appropriate APIs or connectors.  
5. Document the steps and assumptions taken in your code.
6. Use of a free tier on AWS is preferred but if you are more comfortable in another Cloud Provider then use that. Alternatively, you may choose to standup a local Kubernetes cluster and deploy a data warehouse solution.

## Instructions
1. Implement the data pipeline in Python and any additional required files.
2. Include a README.md file with instructions on how to run your code.
